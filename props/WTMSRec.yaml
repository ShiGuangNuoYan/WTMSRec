transformer_layers: 3
num_attention_heads: 4
c_num_heads: 4
hidden_size: 512
c_hidden_dim: 512
inner_size: 256
hidden_dropout: 0.4
f_hidden_size: 512
f_out_dropout: 0.4
c: 5
alpha: 0.7
beta: 0.2
attention_dropout: 0.5
hidden_act: gelu
f_dropout: 0.3 #filter
layer_norm_eps: 1e-12
initializer_range: 0.02
loss_type: CE
have_T : True
cos_decay : True
decay_max: 0
decay_min: 1
decay_loops: 6
t_start: 1
t_end: 20

item_drop_ratio: 0.2
item_drop_coefficient: 0.9
lambda: 1e-3
gamma: 1e-4

plm_suffix: feat1CLS
plm_suffix_aug: feat2CLS
img_suffix: feat3CLS
id_type: id # none / id
modal_type: img_text # none / img / text / img_text
plm_size: 512
img_size: 512 # 768
temperature: 0.07

